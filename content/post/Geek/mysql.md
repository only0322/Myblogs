---
title: "MySQL实战45讲"
date: 2021-01-27T10:36:34+08:00
draft: true
---

## 基础架构

先看MySQL的架构图 

![架构图](/images/geek/mysql/MySQL架构图.png)

大体来说，MySQL 可以分为 Server 层和存储引擎层两部分。

Server 层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。而存储引擎层负责数据的存储和提取。

其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎。

**MySQL的用户建立连接后，如果修改了权限，也得下次重新连接才会更新了。**

数据库里面，长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。（基于tcp的）

长连接会占用内存，直到连接断掉才会释放，所以累积下来的话内存占用会很大。

### 1.SQL查询语句执行过程

连接创建过程见上。

MySQL的查询缓存会非常消耗性能，而且表更新之后就得刷新，所以8.0版本之后将整块功能都删除了。

#### 分析器和优化器

SQL解析器，词法分析。

优化器会从SQL的解析结果，判断有多个索引先用哪个索引，逻辑相同的SQL条件会选择最快的执行方式。

#### 执行器

会判断对某个表是否有查询权限，有权限就打开表执行，根据引擎定义去使用接口。

>有些时候，SQL语句要操作的表不只是SQL字面上那些。比如如果有个触发器，得在执行器阶段（过程中）才能确定。优化器阶段前是无能为力的

如果表 T 中，ID 字段没有索引，那么执行器的执行流程是这样的：

>1.调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中；

>2.调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。

>3.执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。

### 2.更新SQL语句的执行过程

与查询不同的是，更新还涉及两个模块，redo log（重做日志），binlog（归档日志）。

#### redo log

MySQL有用到WAL技术，Write-Ahead Logging，它的关键点就是先写日志，再写磁盘，也就是先写粉板，等不忙的时候再写账本。

具体来说，当有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log（粉板）里面，并更新内存，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做。

InnoDB 的 redo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1GB，那么这块“粉板”总共就可以记录 4GB 的操作。从头开始写，写到末尾就又回到开头循环写。

![redolog示意图](/images/geek/mysql/redolog.png)

数据写满之后，MySQL就会整体将切入点推进以下，擦掉已经写库的记录。

#### binlog 日志模块

这两种日志有以下三点不同。

>redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。

>redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。

>redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。

![update](/images/geek/mysql/update语句流程图.png)

将 redo log 的写入拆成了两个步骤：prepare 和 commit，这就是"两阶段提交"。

#### 两阶段提交

>定义：redo log 等待 binlog 写入完成后，由 prepare 变为 commit 提交状态

先看数据的恢复过程：

当需要恢复到指定的某一秒时，比如某天下午两点发现中午十二点有一次误删表，需要找回数据，那你可以这么做：

首先，找到最近的一次全量备份，如果你运气好，可能就是昨天晚上的一个备份，从这个备份恢复到临时库；

然后，从备份的时间点开始，将备份的 binlog 依次取出来，重放到中午误删表之前的那个时刻。这样你的临时库就跟误删之前的线上库一样了，然后你可以把表数据从临时库取出来，按需要恢复到线上库去。

>redo log和binlog是两个独立的逻辑，如果不用两阶段提交，会出现数据库的状态和日志恢复出来的库状态不一致。

```word
两个阶段的执行

1.请求阶段（commit-request phase，或称表决阶段，voting phase）
在请求阶段，协调者将通知事务参与者准备提交或取消事务，然后进入表决过程。
在表决过程中，参与者将告知协调者自己的决策：同意（事务参与者本地作业执行成功）或取消（本地作业执行故障）。

2.提交阶段（commit phase）
在该阶段，协调者将基于第一个阶段的投票结果进行决策：提交或取消。
当且仅当所有的参与者同意提交事务协调者才通知所有的参与者提交事务，否则协调者将通知所有的参与者取消事务。
参与者在接收到协调者发来的消息后将执行响应的操作。

```

### 3.事务和隔离级别

四种隔离级别：读未提交、读已提交、重复读、串行。

![事务](/images/geek/mysql/事务.png)

**我们来看看在不同的隔离级别下，事务 A 会有哪些不同的返回结果，也就是图里面 V1、V2、V3 的返回值分别是什么。**

>若隔离级别是“读未提交”， 则 V1 的值就是 2。这时候事务 B 虽然还没有提交，但是结果已经被 A 看到了。因此，V2、V3 也都是 2。

>若隔离级别是“读提交”，则 V1 是 1，V2 的值是 2。事务 B 的更新在提交后才能被 A 看到。所以， V3 的值也是 2。

>若隔离级别是“可重复读”，则 V1、V2 是 1，V3 是 2。之所以 V2 还是 1，遵循的就是这个要求：事务在执行期间看到的数据前后必须是一致的。

>若隔离级别是“串行化”，则在事务 B 执行“将 1 改成 2”的时候，会被锁住。直到事务 A 提交后，事务 B 才可以继续执行。所以从 A 的角度看， V1、V2 值是 1，V3 的值是 2。

实现上，数据库会创建一个视图，访问的时候以视图的逻辑为准。

事务执行会生成多个视图，在隔离级别高的时候，要读到之前的事务数据，就必须通过回滚来得到。

当没有事务需要回滚日志的时候，回滚日志会被删除，当系统没有比这个回滚日志更早的视图，就说明可以删除了。

#### 尽量不使用长事务

长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。

在 MySQL 5.5 及以前的版本，回滚日志是跟数据字典一起放在 ibdata 文件里的，即使长事务最终提交，回滚段被清理，文件也不会变小。

曾有数据只有 20GB，而回滚段有 200GB 的库。最终只好为了清理回滚段，重建整个库。

set autocommit=0 的命令会导致事务不自动提交，最后就有可能执行了一个长事务。

**查询执行时间超过60s的事务**

```sql

select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))>60
```

### 4.索引

对于数据库的表而言，索引就相当于目录。

常见的索引数据结构：哈希表，有序数组，搜索树。

#### 哈希表

哈希表是一种以键 - 值（key-value）存储数据的结构，我们只要输入待查找的键即 key，就可以找到其对应的值即 Value。

哈希的思路很简单，把值放在数组里，用一个哈希函数把 key 换算成一个确定的位置，然后把 value 放在数组的这个位置。

不可避免地，多个 key 值经过哈希函数的换算，会出现同一个值的情况。处理这种情况的一种方法是，拉出一个链表。

![哈希表](/images/geek/mysql/哈希表.png)

哈希表只适合等值查询。

#### 有序数组

只适合静态存储引擎，比如存储某年某个城市的所有人口信息，这类不会被修改的数据。

身份证号递增排序，二分查找可以很快的查到某个人的信息。

但有序数组完全不适合插入和删除操作，所以最好只用于静态存储引擎。

#### 二叉搜索树

保持树枝节点的左子树都比节点小，右子树都比节点大，某种程度的二分查找。但维护平衡的时候会比较耗时。

为什么数据库存储使用b+树 而不是二叉树，因为二叉树树高过高，每次查询都需要访问过多节点，即访问数据块过多，而从磁盘随机读取数据块过于耗时。

数据库底层存储的核心就是基于这些数据模型的。每碰到一个新数据库，我们需要先关注它的数据模型，这样才能从理论上分析出这个数据库的适用场景。

#### 回表

```sql

mysql> create table T (
ID int primary key,
k int NOT NULL DEFAULT 0, 
s varchar(16) NOT NULL DEFAULT '',
index k(k))
engine=InnoDB;

insert into T values(100,1, 'aa'),(200,2,'bb'),(300,3,'cc'),(500,5,'ee'),(600,6,'ff'),(700,7,'gg');
```

`select * from T where k between 3 and 5`

一起来看看这条 SQL 查询语句的执行流程：

1. 在 k 索引树上找到 k=3 的记录，取得 ID = 300；

2. 再到 ID 索引树查到 ID=300 对应的 R3；

3. 在 k 索引树取下一个值 k=5，取得 ID=500；

4. 再回到 ID 索引树查到 ID=500 对应的 R4；

5. 在 k 索引树取下一个值 k=6，不满足条件，循环结束。

在这个过程中，回到主键索引树搜索的过程，我们称为回表。回表也就是值不在索引树上面，需要从表里进行查询。



#### 最左前缀原则

索引项是按照索引定义里面出现的字段顺序排序的。

当你的逻辑需求是查到所有名字是“张三”的人时，可以快速定位到 ID4，然后向后遍历得到所有需要的结果。

如果你要查的是所有名字第一个字是“张”的人，你的 SQL 语句的条件是"where name like ‘张 %’"。

这时，你也能够用上这个索引，查找到第一个符合条件的记录是 ID3，然后向后遍历，直到不满足条件为止。

#### 索引下推

索引下推（Index Condition Pushdown），简称 ICP。 是Mysql 5.6版本引入的技术优化。

旨在 在“仅能利用最左前缀索的场景”下（而不是能利用全部联合索引），对不在最左前缀索引中的其他联合索引字段加以利用——在遍历索引时，就用这些其他字段进行过滤(where条件里的匹配)。

过滤会减少遍历索引查出的主键条数，从而减少回表次数，提示整体性能。

### 5.锁

根据加锁的范围，MySQL 里面的锁大致可以分成全局锁、表级锁和行锁三类。

#### 全局锁

全局锁的典型使用场景是，做全库逻辑备份。也就是把整库每个表都 select 出来存成文本。

#### 表级锁

MySQL里面的表级锁共两种，表锁和元数据锁。

举个例子, 如果在某个线程 A 中执行 lock tables t1 read, t2 write; 这个语句，则其他线程写 t1、读写 t2 的语句都会被阻塞。
、
同时，线程 A 在执行 unlock tables 之前，也只能执行读 t1、读写 t2 的操作。连写 t1 都不允许，自然也不能访问其他表。

**元数据锁**

在 MySQL 5.5 版本中引入了 MDL，当对一个表做增删改查操作的时候，加 MDL 读锁；

当要对表做结构变更操作的时候，加 MDL 写锁。读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。

读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。

因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。

![死锁](/images/geek/mysql/加字段死锁.png)

>我们可以看到 session A 先启动，这时候会对表 t 加一个 MDL 读锁。

>由于 session B 需要的也是 MDL 读锁，因此可以正常执行。

>之后 session C 会被 blocked，是因为 session A 的 MDL 读锁还没有释放，而 session C 需要 MDL 写锁，因此只能被阻塞。

>如果只有 session C 自己被阻塞还没什么关系，但是之后所有要在表 t 上新申请 MDL 读锁的请求也会被 session C 阻塞。

>前面我们说了，所有对表的增删改查操作都需要先申请 MDL 读锁，就都被锁住，等于这个表现在完全不可读写了。

>如果某个表上的查询语句频繁，而且客户端有重试机制，也就是说超时后会再起一个新 session 再请求的话，这个库的线程很快就会爆满。

>你现在应该知道了，事务中的 MDL 锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放。

#### 二阶段锁

在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。

#### 死锁

死锁简单来说就是两个事务在等待对方释放锁，造成了死锁。

死锁解决策略：

1. 直接等待，直到超时。

缺点在于无法准确地估算等待时间，太长的话业务系统可能是无法接受的缓慢，短的话可能会造成误伤。

2. 死锁检测。

发现死锁，主动回滚死锁链条的某一个事务，让其他事务得以继续进行。

所有事务都要更新同一行的场景呢：

>每个新来的被堵住的线程，都要判断会不会由于自己的加入导致了死锁，这是一个时间复杂度是 O(n) 的操作。

>假设有 1000 个并发线程要同时更新同一行，那么死锁检测操作就是 100 万这个量级的。

>虽然最终检测的结果是没有死锁，但是这期间要消耗大量的 CPU 资源。

>因此，你就会看到 CPU 利用率很高，但是每秒却执行不了几个事务。

这个并发控制要做在数据库服务端。如果你有中间件，可以考虑在中间件实现；

如果你的团队有能修改 MySQL 源码的人，也可以做在 MySQL 里面。

基本思路就是，对于相同行的更新，在进入引擎之前排队。

这样在 InnoDB 内部就不会有大量的死锁检测工作了。

```
你可以考虑通过将一行改成逻辑上的多行来减少锁冲突。

还是以影院账户为例，可以考虑放在多条记录上，比如 10 个记录，影院的账户总额等于这 10 个记录的值的总和。

这样每次要给影院账户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成原来的 1/10，可以减少锁等待个数，也就减少了死锁检测的 CPU 消耗。

```

